{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba64a01-486b-465d-ba3c-8775e033aeb4",
   "metadata": {},
   "source": [
    "# 距離学習\n",
    "入力：OSNetから得た外観特徴量ベクトル512次元  \n",
    "出力：圧縮した外観特徴量ベクトル128次元  \n",
    "損失関数：triplet loss  \n",
    "\n",
    "訓練データ，検証データ，テストデータは以下の３通り．各データセットにおいて以下のコードを繰り返す  \n",
    "\n",
    "- 訓練データ，検証データ：撮影実験１，２　，テストデータ：撮影実験３\n",
    "- 訓練データ，検証データ：撮影実験２，３　，テストデータ：撮影実験１\n",
    "- 訓練データ，検証データ：撮影実験３，１　，テストデータ：撮影実験２"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e4c22-14cc-41c1-a1f9-d6f48860d8a0",
   "metadata": {},
   "source": [
    "## ライブラリとパラメーター\n",
    "\n",
    "- 訓練：検証：テスト = 6：2：2としている．（訓練データ，検証データとテストデータは事前に分けているので本当は訓練：検証 = 8:2でいいはず．このコードでは訓練データを全て使い切れていない．要修正）\n",
    "- 距離定義　コサイン類似度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25b78ab-75f3-4564-b3a5-be3338126229",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Subset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\"\"\" ライブラリ読込 \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from pytorch_metric_learning.miners import TripletMarginMiner\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.losses import TripletMarginLoss\n",
    "from pytorch_metric_learning.reducers import ThresholdReducer\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, distances, reducers, samplers\n",
    "from pytorch_metric_learning import losses, distances, regularizers\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "# データ作成のパラメーター\n",
    "TRAIN_TEST_RATIO = 0.2\n",
    "TRAIN_VAL_RATIO = 0.2\n",
    "exclusion_num = 99\n",
    "\n",
    "# 学習のパラメーター\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# デフォルトのファイル保存先, 名前\n",
    "save_path = \"/content/\"\n",
    "file_name = \"動作確認\"\n",
    "\n",
    "# TripletMarginLoss\n",
    "distance = CosineSimilarity()\n",
    "reducer = ThresholdReducer(low = 0)\n",
    "loss_func = TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)\n",
    "mining_func = TripletMarginMiner(margin=0.2, distance=distance, type_of_triplets=\"semi-hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7c4ac-7df1-45e0-8afa-504287d89ac1",
   "metadata": {},
   "source": [
    "## データ作成関数\n",
    "\n",
    "全ての関数を使用しているわけではない．\n",
    "特に今回の研究では，全て一括のコードで実行しているわけではない．以下の関数を使用して事前に違うコードにて，テストデータからオーギュメンテーション加工を消したりしている．（全てのコードを公開するのは疲れるので，載せません）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b4ecf-ee1a-487f-a921-6e7630428c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time(df):\n",
    "  df_exp1 = df[df['file_name'].str.contains('exp1')]\n",
    "  df_exp2 = df[df['file_name'].str.contains('exp2')]\n",
    "  df_exp3 = df[df['file_name'].str.contains('exp3')]\n",
    "  return df_exp1, df_exp2, df_exp3\n",
    "\n",
    "\n",
    "def split_point(df):\n",
    "  df_point1 = df[df['file_name'].str.contains('point1')]\n",
    "  df_point2 = df[df['file_name'].str.contains('point2')]\n",
    "  df_point3 = df[df['file_name'].str.contains('point3')]\n",
    "  df_point4 = df[df['file_name'].str.contains('point4')]\n",
    "  df_point5 = df[df['file_name'].str.contains('point5')]\n",
    "  df_point6 = df[df['file_name'].str.contains('point6')]\n",
    "  return df_point1, df_point2, df_point3, df_point4, df_point5, df_point6\n",
    "\n",
    "def split_person(df):\n",
    "  df_others = df[df['label'] == 0]\n",
    "  df_ichimanda = df[df['label'] == 1]\n",
    "  df_ichiraku = df[df['label'] == 2]\n",
    "  df_idogawa = df[df['label'] == 3]\n",
    "  df_miyazaki = df[df['label'] == 4]\n",
    "  df_noguchi = df[df['label'] == 5]\n",
    "  df_sakai = df[df['label'] == 6]\n",
    "  return df_others, df_ichimanda, df_ichiraku, df_idogawa, df_miyazaki, df_noguchi, df_sakai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d409dff-bcba-43a3-bdbf-11e780001e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_augmented_data_choice(df, del_aug_list):\n",
    "  # DataFrameのbool値の反転には「~」を使う　https://itips.krsw.biz/pandas-dataframe-how-to-reverse-bool-column/\n",
    "  aug_dict = {\"blur\":'_0.jpg', \"bright\":'_1.jpg', \"contrast\":'_３.jpg', \"contrast_flip\":'_7.jpg'}\n",
    "  for aug in del_aug_list:\n",
    "    df = df[~df['file_name'].str.contains(aug_dict[aug])]\n",
    "  return df\n",
    "\n",
    "\n",
    "def del_augmented_data_all(df):\n",
    "  # テストのデータを元データのみにする（テストデータに水増しにが入るのはおかしいから）\n",
    "  # DataFrameのbool値の反転には「~」を使う　https://itips.krsw.biz/pandas-dataframe-how-to-reverse-bool-column/\n",
    "  aug_list = ['_0.jpg', '_1.jpg', '_3.jpg' , '_7.jpg']\n",
    "  for aug in aug_list:\n",
    "    df = df[~df['file_name'].str.contains(aug)]\n",
    "  return df\n",
    "\n",
    "\n",
    "def del_duplication_data(df_train, df_test):\n",
    "  file_name_list = df_test['file_name'].tolist()\n",
    "  for i, file_name in enumerate(file_name_list):\n",
    "    df_train = df_train[~df_train['file_name'].str.contains(file_name)]\n",
    "    print(i)\n",
    "  return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb19f12-5b79-4523-a81b-6827dfe110ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "学習データにおいて，すべてのラベルのサンプル数が同数になるように，min_randam_samplingを用いて，サンプル数が最小のラベルに合わせてランダムサンプリングをする\n",
    "\n",
    "その後，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691763f-d0f5-4883-af12-f93cf0d14575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_randam_sampling(df):\n",
    "  # dfを最小のラベルの個数に合わせてそれぞれランダムサンプリング\n",
    "  number = df['label'].value_counts().min()\n",
    "  filename_list = df['label'].unique()\n",
    "  df_temp = pd.DataFrame()\n",
    "  for i in filename_list:\n",
    "    df_p = df.groupby(\"label\").get_group(i).sample(n=number)\n",
    "    # 結合\n",
    "    df_temp = pd.concat([df_temp, df_p])\n",
    "  df = df_temp\n",
    "  df.shape\n",
    "  print(\"最小のラベルの個数に合わせてそれぞれランダムサンプリングしました\")\n",
    "  return df\n",
    "\n",
    "\n",
    "def del_exclusion_num_person(df, exclusion_num):\n",
    "  # 条件に応じて一部のデータを除外する\n",
    "  #99はその他を含む 0はその他だけ除外, 1以降はその他と番号の被験者を除外\n",
    "  exclusion_num = exclusion_num\n",
    "  number = df['label'].value_counts().min()\n",
    "  filename_list = df['label'].unique()\n",
    "\n",
    "\n",
    "  if exclusion_num == 99:\n",
    "    print(\"その他の人を含むデータセットです\")\n",
    "\n",
    "    # label:0を削減\n",
    "    number = 5000\n",
    "    df_s = df.groupby(\"label\").get_group(0).sample(n=number)\n",
    "    df_t = df[df['label'] != 0]\n",
    "    # 結合\n",
    "    df_sample = pd.concat([df_s, df_t])\n",
    "    df_sample\n",
    "\n",
    "  elif exclusion_num == 0:\n",
    "    print(\"その他の人を抜いたデータセットです\")\n",
    "    df_sample = df[df['label'] != 0]\n",
    "    df_sample[\"label\"] = df_sample[\"label\"] -1\n",
    "\n",
    "  else:\n",
    "    print(\"label\"+str(exclusion_num)+\"の人を抜いたデータセットです\")\n",
    "    for i in filename_list:\n",
    "      if i == 0:\n",
    "        continue\n",
    "      elif i < exclusion_num:\n",
    "        df_p = df.groupby(\"label\").get_group(i).sample(n=number)\n",
    "        df_p[\"label\"] = df_p[\"label\"] -1\n",
    "        df_sample = pd.concat([df_sample, df_p])\n",
    "      elif i == exclusion_num:\n",
    "        continue\n",
    "      elif i > exclusion_num:\n",
    "        df_p = df.groupby(\"label\").get_group(i).sample(n=number)\n",
    "        df_p[\"label\"] = df_p[\"label\"] -2\n",
    "        df_sample = pd.concat([df_sample, df_p])\n",
    "\n",
    "\n",
    "  df = df_sample\n",
    "  print(df.iloc[:, 0].value_counts(sort=False))\n",
    "  return df, exclusion_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75229dfc-2624-448b-a3f1-5cf1a56ee064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ded3b-0d9b-4349-ab97-021fb69f2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_train_test(df):\n",
    "  ### 訓練からテストを切り出す場合\n",
    "  df_train = df\n",
    "  df_train, df_test = train_test_split(df_train, test_size=TRAIN_TEST_RATIO, random_state=0)\n",
    "\n",
    "  df_train.columns = df.columns.values\n",
    "  df_test.columns = df.columns.values\n",
    "  return df_train, df_test\n",
    "\n",
    "\n",
    "def make_X_y(df_train, df_test):\n",
    "  ### 訓練とテストを別々に用意する場合\n",
    "\n",
    "  # 特徴量 (説明変数) 訓練\n",
    "  X_train = df_train.iloc[:, 3:]\n",
    "  # 特徴量 (説明変数)テスト\n",
    "  X_test = df_test.iloc[:, 3:]\n",
    "  # ラベル（目的変数)\n",
    "  y_train = df_train.iloc[:, 0]\n",
    "  # ラベル（目的変数)\n",
    "  y_test = df_test.iloc[:, 0]\n",
    "\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def make_dataloader(X_train, X_test, y_train, y_test):\n",
    "  print(\"----------------------\")\n",
    "  X_train,X_val,y_train,y_val = train_test_split(X_train, y_train, test_size=TRAIN_VAL_RATIO, random_state=0, stratify=y_train)\n",
    "\n",
    "\n",
    "  print(\"----------------------\")\n",
    "  print(\"trainに含まれる各labelの個数\")\n",
    "  print(y_train.value_counts(sort=True))\n",
    "  print(\"----------------------\")\n",
    "  print(\"valに含まれる各labelの個数\")\n",
    "  print(y_val.value_counts(sort=True))\n",
    "  print(\"----------------------\")\n",
    "  print(\"testに含まれる各labelの個数\")\n",
    "  print(y_test.value_counts(sort=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1365d-4a8a-4e3d-bd86-8a9ada5c778f",
   "metadata": {},
   "source": [
    "GPU上でpytorchを用いる場合は，Pytorch tensorに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd1eab-1d52-41aa-8e1d-531363eb3373",
   "metadata": {},
   "outputs": [],
   "source": [
    " '''PyTorch tensorへ変換'''\n",
    "  # https://megane-man666.hatenablog.com/entry/pytorch\n",
    "  # PyTorchのデータセットは、numpy型を取り込んで、tensor型でする必要があります。\n",
    "  # そこで、.valuesを使ってDataFrame型からnumpy.ndarray型に変換して、torch.Tesor()、LongTensor()でtensor型に変換しています。\n",
    "  # 説明変数をTensor()、目的変数をLongTensor()で変換しないと、RuntimeErrorが発生しますので間違えないようにしましょう。\n",
    "  X_train = torch.Tensor(X_train.values)\n",
    "  X_val = torch.Tensor(X_val.values)\n",
    "  X_test = torch.Tensor(X_test.values)\n",
    "  y_train = torch.LongTensor(y_train.values)\n",
    "  y_val = torch.LongTensor(y_val.values)\n",
    "  y_test = torch.LongTensor(y_test.values)\n",
    "\n",
    "  train_data = TensorDataset(X_train, y_train)\n",
    "  val_data = TensorDataset(X_train, y_train)\n",
    "  test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "  # https://kdl-di.hatenablog.com/entry/2021/12/24/090000\n",
    "  # 一般的に機械学習では、学習用データ、検証用データ、評価用データの三種類を要します。\n",
    "  # 学習用データ：機械学習モデルが学習する際に利用するデータ。複数回利用する\n",
    "  # 検証用データ：学習データで学習したモデルが、どの程度汎化性能を持っているか評価する際に利用するデータ。複数回利用する\n",
    "  # 評価用データ：学習したモデルが未知のデータに対してどれだけ適応できるかを検証する。学習したモデルに対して一度のみ利用する\n",
    "\n",
    "  train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "  val_loader = DataLoader(val_data, batch_size)\n",
    "  test_loader = DataLoader(test_data)\n",
    "\n",
    "  return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b78d6-e7d3-498e-b9c4-99cf4dd685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_file_name(df):\n",
    "  print(str(df) + \"に含まれる画像の名前です\")\n",
    "  for i in df[\"file_name\"]:\n",
    "    print(i)\n",
    "\n",
    "def show_label_count(df):\n",
    "  print(\"testに含まれる各labelの個数\")\n",
    "  print(df.iloc[:, 0].value_counts(sort=True))\n",
    "\n",
    "\n",
    "# モデルの定義\n",
    "class Net(nn.Module):\n",
    "\n",
    "    # ユニット・層の数・活性化関数等ニューラルネットワークの模型となるものを下記に記述\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.fc3 = nn.Linear(300, 200)\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(200, 100)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.fc6 = nn.Linear(50, 25)\n",
    "        self.bn3 = nn.BatchNorm1d(25)\n",
    "        self.fc7 = nn.Linear(25, 20)\n",
    "        self.fc8 = nn.Linear(20, 2)\n",
    "\n",
    "    # 順方向の計算処理の流れを下記に記述\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU: max(x, 0)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6f5f4-563b-4f08-af31-e44369eb1273",
   "metadata": {},
   "source": [
    "# 学習用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee74fff-e6a0-4c9d-ba36-24ba29fd8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用関数\n",
    "def train(model, loss_func, mining_func, device, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(inputs)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    #     if idx % 10 == 0:\n",
    "    #         print('Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}'.format(epoch, idx, loss, mining_func.num_triplets))\n",
    "    # print()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# バリデーション用関数\n",
    "def val(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            embeddings = model(inputs)\n",
    "            indices_tuple = mining_func(embeddings, labels)\n",
    "            loss = loss_func(embeddings, labels, indices_tuple)\n",
    "            loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# テスト用関数\n",
    "def test(model, dataloader, device):\n",
    "    predicted_metrics = []\n",
    "    true_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            metric = model(inputs).detach().cpu().numpy()\n",
    "            metric = metric.reshape(metric.shape[0], metric.shape[1])\n",
    "            predicted_metrics.append(metric)\n",
    "            true_labels.append(labels.detach().cpu().numpy())\n",
    "    return np.concatenate(predicted_metrics), np.concatenate(true_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbe13-9232-4db2-b6f2-6b42c60a81de",
   "metadata": {},
   "source": [
    "# 学習用関数\n",
    "\n",
    "### 実際に使用した関数\n",
    "- run_PML_128\n",
    "- train()\n",
    "- val()\n",
    "- read_csv()\n",
    "- make_exp1_aug()　（名前が悪い！！exp1もaugも関係なし．特徴量データを訓練．検証，テストに分ける関数）\n",
    "- make_dataloder()\n",
    "\n",
    "結局使用している関数は少ないね…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d90ab5-f731-4aeb-95cb-15ad2d445692",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 学習実行する関数\n",
    "\n",
    "def run_PML(save_path, file_name):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 学習と評価\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    best_loss = None\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    # val_lossとtrain_loss両方で判定(train_lossが十分に下がっていないのに学習終了するのを防ぐ)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        # print('-' * 10)\n",
    "        train_loss = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "        val_loss = val(model, val_loader, device, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        ## best_lossより小さなval_lossが出力された場合のみモデルを保存する。##\n",
    "        if best_loss is None:\n",
    "            best_loss = val_loss\n",
    "\n",
    "        ## val_loss最小が更新されたらモデルを保存\n",
    "        elif val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), './best_model_PML_exp1.pth')\n",
    "            print(str(best_loss_epoch) + \"エポック目で最小のval_loss:\" + str(best_loss) + \"を更新したため、モデルを保存しました\")\n",
    "            best_loss_epoch = epoch + 1\n",
    "\n",
    "        ## best_lossが0になったら終了。##\n",
    "        elif (best_loss == 0) and (train_loss < 0.5):\n",
    "            torch.save(model.state_dict(), './best_model_PML_exp1.pth')\n",
    "            best_loss_epoch = epoch + 1\n",
    "            break\n",
    "\n",
    "    print(str(best_loss_epoch - 1) + \"エポック目が最小のval_loss=\" + str(best_loss) + \"でした\")\n",
    "\n",
    "    # # loss_historyを表示して保存\n",
    "    show_loss_history(train_loss_list, val_loss_list, save_path, file_name)\n",
    "\n",
    "    # # モデルの出力が2次元のとき\n",
    "    df_extract_train_PML = extract_feature_PML(train_loader)\n",
    "    df_extract_test_PML = extract_feature_PML(test_loader)\n",
    "    show_PML_output(df_extract_train_PML, df_extract_test_PML, save_path, file_name)\n",
    "\n",
    "    return df_extract_train_PML, df_extract_test_PML\n",
    "\n",
    "    # モデルの出力が多次元のとき\n",
    "    # 特徴量抽出\n",
    "    df_extract_train_PML = extract_feature_PML(train_loader)\n",
    "    df_extract_test_PML = extract_feature_PML(test_loader)\n",
    "\n",
    "    # UMAPで2次元に圧縮\n",
    "    df_extract_train_PML_UMAP, df_extract_test_PML_UMAP = apply_UMAP(df_extract_train_PML, df_extract_test_PML)\n",
    "\n",
    "    # 表示\n",
    "    show_PML_output(df_extract_train_PML_UMAP, df_extract_test_PML_UMAP, save_path, file_name)\n",
    "\n",
    "    return df_extract_train_PML_UMAP, df_extract_test_PML_UMAP### 学習実行する関数\n",
    "\n",
    "def run_PML(save_path, file_name):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 学習と評価\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    best_loss = None\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    # val_lossとtrain_loss両方で判定(train_lossが十分に下がっていないのに学習終了するのを防ぐ)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        # print('-' * 10)\n",
    "        train_loss = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "        val_loss = val(model, val_loader, device, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        ## best_lossより小さなval_lossが出力された場合のみモデルを保存する。##\n",
    "        if best_loss is None:\n",
    "            best_loss = val_loss\n",
    "\n",
    "        ## val_loss最小が更新されたらモデルを保存\n",
    "        elif val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), './best_model_PML_exp1.pth')\n",
    "            print(str(best_loss_epoch) + \"エポック目で最小のval_loss:\" + str(best_loss) + \"を更新したため、モデルを保存しました\")\n",
    "            best_loss_epoch = epoch + 1\n",
    "\n",
    "        ## best_lossが0になったら終了。##\n",
    "        elif (best_loss == 0) and (train_loss < 0.5):\n",
    "            torch.save(model.state_dict(), './best_model_PML_exp1.pth')\n",
    "            best_loss_epoch = epoch + 1\n",
    "            break\n",
    "\n",
    "    print(str(best_loss_epoch - 1) + \"エポック目が最小のval_loss=\" + str(best_loss) + \"でした\")\n",
    "\n",
    "    # # loss_historyを表示して保存\n",
    "    show_loss_history(train_loss_list, val_loss_list, save_path, file_name)\n",
    "\n",
    "    # # モデルの出力が2次元のとき\n",
    "    df_extract_train_PML = extract_feature_PML(train_loader)\n",
    "    df_extract_test_PML = extract_feature_PML(test_loader)\n",
    "    show_PML_output(df_extract_train_PML, df_extract_test_PML, save_path, file_name)\n",
    "\n",
    "    return df_extract_train_PML, df_extract_test_PML\n",
    "\n",
    "    # モデルの出力が多次元のとき\n",
    "    # 特徴量抽出\n",
    "    df_extract_train_PML = extract_feature_PML(train_loader)\n",
    "    df_extract_test_PML = extract_feature_PML(test_loader)\n",
    "\n",
    "    # UMAPで2次元に圧縮\n",
    "    df_extract_train_PML_UMAP, df_extract_test_PML_UMAP = apply_UMAP(df_extract_train_PML, df_extract_test_PML)\n",
    "\n",
    "    # 表示\n",
    "    show_PML_output(df_extract_train_PML_UMAP, df_extract_test_PML_UMAP, save_path, file_name)\n",
    "\n",
    "    return df_extract_train_PML_UMAP, df_extract_test_PML_UMAP\n",
    "\n",
    "\n",
    "class Net128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net128, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 384)\n",
    "        self.fc2 = nn.Linear(384, 288)  # 調整\n",
    "        self.bn1 = nn.BatchNorm1d(288)  # 調整\n",
    "        self.fc3 = nn.Linear(288, 224)  # 調整\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(224, 160)  # 調整\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(160, 128)  # 調整\n",
    "        self.bn2 = nn.BatchNorm1d(128)  # 調整\n",
    "        self.fc6 = nn.Linear(128, 128)  # 最終層に近づくため同じ次元数を保持\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc6(x)  # 最終層\n",
    "        return x\n",
    "##############################################################################\n",
    "######################################################################\n",
    "### 学習実行する関数(モデル学習のみ）\n",
    "\n",
    "def run_PML128(save_path, file_name):\n",
    "    model = Net128().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 学習と評価\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    best_loss = None\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    # val_lossとtrain_loss両方で判定(train_lossが十分に下がっていないのに学習終了するのを防ぐ)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        # print('-' * 10)\n",
    "        train_loss = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "        val_loss = val(model, val_loader, device, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        ## best_lossより小さなval_lossが出力された場合のみモデルを保存する。##\n",
    "        if best_loss is None:\n",
    "            best_loss = val_loss\n",
    "\n",
    "        ## val_loss最小が更新されたらモデルを保存\n",
    "        elif val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), './best_model_PML128_point5_kouzumi.pth')\n",
    "            print(str(best_loss_epoch) + \"エポック目で最小のval_loss:\" + str(best_loss) + \"を更新したため、モデルを保存しました\")\n",
    "            best_loss_epoch = epoch + 1\n",
    "\n",
    "        ## best_lossが0になったら終了。##\n",
    "        elif (best_loss == 0) and (train_loss < 0.5):\n",
    "            torch.save(model.state_dict(), './best_model_PML128_point5_koizumi.pth')\n",
    "            best_loss_epoch = epoch + 1\n",
    "            break\n",
    "\n",
    "    print(str(best_loss_epoch - 1) + \"エポック目が最小のval_loss=\" + str(best_loss) + \"でした\")\n",
    "\n",
    "    return best_loss_epoch - 1, best_loss\n",
    "\n",
    "\n",
    "def run_logistic_regression(X_train, X_test, y_train, y_test, save_path, file_name):\n",
    "    lr = LogisticRegression()  # ロジスティック回帰モデルのインスタンスを作成\n",
    "    lr.fit(X_train, y_train)  # ロジスティック回帰モデルの重みを学習\n",
    "\n",
    "    Y_pred = lr.predict(X_test)\n",
    "    df_report_LR = show_report(y_test, Y_pred, save_path, file_name)\n",
    "    return df_report_LR\n",
    "\n",
    "\n",
    "### 精度検証関数\n",
    "\n",
    "def extract_feature_PML(data_loader):\n",
    "    # モデルの再定義\n",
    "    model = Net().to(device)\n",
    "    # 学習済みモデルの読み出し\n",
    "    model.load_state_dict(torch.load('./best_model_PML.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    metric, label = test(model, data_loader, device)\n",
    "\n",
    "    metrics = pd.DataFrame(metric)\n",
    "    label = pd.DataFrame(label)\n",
    "\n",
    "    df_extract = pd.concat([label, metrics], axis=1)\n",
    "\n",
    "    return df_extract\n",
    "\n",
    "\n",
    "def extract_feature_DNN(data_loader):\n",
    "    # モデルの再定義\n",
    "    model = Net().to(device)\n",
    "    # 学習済みモデルの読み出し\n",
    "    model.load_state_dict(torch.load('./best_model_DNN.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    df_extract = pd.DataFrame()\n",
    "    for i, (data, label) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.numpy().tolist()\n",
    "\n",
    "        features = model(data)\n",
    "        label_df = pd.Series(label)\n",
    "\n",
    "        features_np = features.to('cpu').detach().numpy().copy()\n",
    "        features_df = pd.DataFrame(features_np)\n",
    "\n",
    "        features_df = pd.concat([label_df, features_df], axis=1)\n",
    "\n",
    "        df_extract = df_extract.append(features_df)\n",
    "\n",
    "    pd.options.display.float_format = '{:.2f}'.format\n",
    "    df_extract = df_extract.reset_index(drop=True)\n",
    "    print(df_extract)\n",
    "    return df_extract\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def apply_UMAP(df_extract_train, df_extract_test):\n",
    "    # 　高次元の特徴量を教師ありUMAPで2次元にして返す関数\n",
    "\n",
    "    UMAP_X_train = df_extract_train.iloc[:, 1:]\n",
    "    UMAP_X_test = df_extract_test.iloc[:, 1:]\n",
    "    UMAP_Y_train = df_extract_train.iloc[:, 0]\n",
    "    UMAP_Y_test = df_extract_test.iloc[:, 0]\n",
    "\n",
    "    # UMAP\n",
    "    # 訓練とテストを同じ空間に投射したいときは、fit_transformとtransformを使い分ける\n",
    "    # https://aotamasaki.hatenablog.com/entry/2018/07/28/220102\n",
    "\n",
    "    umap = UMAP(n_components=2, random_state=0, n_neighbors=100, metric=\"cosine\")\n",
    "    embedding_train = umap.fit_transform(UMAP_X_train, UMAP_Y_train)  # 教師あり\n",
    "    # embedding_train = umap.fit_transform(UMAP_X_train) # 教師なし\n",
    "    embedding_test = umap.transform(UMAP_X_test)\n",
    "\n",
    "    embedding_train = pd.DataFrame(embedding_train)\n",
    "    embedding_test = pd.DataFrame(embedding_test)\n",
    "\n",
    "    df_extract_train = pd.concat([UMAP_Y_train, embedding_train], axis=1)\n",
    "    df_extract_test = pd.concat([UMAP_Y_test, embedding_test], axis=1)\n",
    "\n",
    "    return df_extract_train, df_extract_test\n",
    "\n",
    "\n",
    "# 深層学習モデルで使った訓練データを使ってknnを訓練、テストデータを投入して予測ラベルを出力\n",
    "def apply_kNN(df_extract_train, df_extract_test):\n",
    "    kNN_X_train = df_extract_train.iloc[:, 1:]\n",
    "    kNN_X_test = df_extract_test.iloc[:, 1:]\n",
    "    kNN_Y_train = df_extract_train.iloc[:, 0]\n",
    "    kNN_Y_test = df_extract_test.iloc[:, 0]\n",
    "\n",
    "    kNN = KNeighborsClassifier(n_neighbors=5)\n",
    "    kNN.fit(kNN_X_train, kNN_Y_train)\n",
    "\n",
    "    kNN_Y_pred = kNN.predict(kNN_X_test)\n",
    "\n",
    "    return kNN_Y_test, kNN_Y_pred\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def show_loss_history(train_loss_list, val_loss_list, save_path, file_name):\n",
    "    fig_loss_history = plt.figure(figsize=(20, 10))\n",
    "    plt.plot(train_loss_list, alpha=0.8, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_list, alpha=0.8, label=\"Val Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig_loss_history.savefig(save_path + file_name + \"_\" + \"loss_history\")\n",
    "\n",
    "\n",
    "def show_2D_metric(df_extract, title, save_path, file_name):\n",
    "    # 散布図\n",
    "    df_label = df_extract.iloc[:, 0]\n",
    "    num_list = np.sort(df_label.unique())\n",
    "\n",
    "    # matplotlibでのラベルの付け方\n",
    "    # https://blog.amedama.jp/entry/umap\n",
    "    fig_2D_metric = plt.figure(figsize=(10, 10))\n",
    "    for n in num_list:\n",
    "        plt.scatter(df_extract[df_extract.iloc[:, 0] == n].iloc[:, 1],\n",
    "                    df_extract[df_extract.iloc[:, 0] == n].iloc[:, 2],\n",
    "                    label=n, alpha=0.8)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig_2D_metric.savefig(save_path + file_name + \"_\" + title)\n",
    "\n",
    "\n",
    "def show_report(label_list, pred_list, save_path, file_name):\n",
    "    # 機械学習の分類評価指標 混同行列と正解率・適合率・再現率・F値\n",
    "    # https://laid-back-scientist.com/confusion-matrix\n",
    "\n",
    "    # 【sklearn】Classification_reportの使い方を丁寧に\n",
    "    # https://gotutiyan.hatenablog.com/entry/2020/09/09/111840\n",
    "\n",
    "    # 混同行列を作成\n",
    "    cm = confusion_matrix(label_list, pred_list)\n",
    "    # https://evaluelog.com/post-122/\n",
    "    index = [\"others\", \"ichimanda\", \"ichiraku\", \"idogawa\", \"miyazaki\", \"noguchi\", \"sakai\"]\n",
    "    columns = [\"others\", \"ichimanda\", \"ichiraku\", \"idogawa\", \"miyazaki\", \"noguchi\", \"sakai\"]\n",
    "    if exclusion_num == 0:\n",
    "        del index[0]  # othersを削除\n",
    "        del columns[0]  # othersを削除\n",
    "    elif exclusion_num == 99:\n",
    "        pass\n",
    "    elif exclusion_num != 0:\n",
    "        del index[0]  # othersを削除\n",
    "        del columns[0]  # othersを削除\n",
    "        del index[exclusion_num - 1]  # 指定した番号の人を除外(上でothersを削除したので1ずれる)\n",
    "        del columns[exclusion_num - 1]  # 指定した番号の人を除外(上でothersを削除したので1ずれる)\n",
    "    cm = pd.DataFrame(data=cm, index=index, columns=columns)\n",
    "\n",
    "    # 混同行列を可視化\n",
    "    # Seabornでheatmapを表示させた時の数字を1.5e+02→150に変更[Python]\n",
    "    # https://onl.bz/zMYfsFy\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "    glaph = sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='d')  # 「fmt='d'」をつける\n",
    "    glaph.set(xlabel=\"predict\", ylabel=\"label\")\n",
    "    # 画像で保存\n",
    "    fig = glaph.get_figure()\n",
    "    fig.savefig(save_path + file_name + \"_\" + \"cm\" + \".png\")\n",
    "\n",
    "    # Classification_reportを表示\n",
    "    pd.options.display.float_format = '{:.4g}'.format\n",
    "    eval_dict = classification_report(label_list, pred_list, target_names=columns, output_dict=True)\n",
    "    df_report = pd.DataFrame(eval_dict)  # DataFrameとして表示\n",
    "    # csvで保存\n",
    "    df_report.to_csv(save_path + file_name + \"_\" + \"report\" + \".csv\")\n",
    "\n",
    "    print(df_report)\n",
    "\n",
    "    return df_report\n",
    "\n",
    "\n",
    "def show_PML_output(df_extract_train_PML, df_extract_test_PML, save_path, file_name):\n",
    "    # 訓練データを入力したときの出力を可視化\n",
    "    show_2D_metric(df_extract_train_PML, \"PML-train-metric\", save_path, file_name)\n",
    "    # テストデータを入力したときの出力を可視化\n",
    "    show_2D_metric(df_extract_test_PML, \"PML-test-metric\", save_path, file_name)\n",
    "\n",
    "    # 精度評価(2次元ベクトルにkNN)\n",
    "    kNN_Y_test_PML, kNN_Y_pred_PML = apply_kNN(df_extract_train_PML, df_extract_test_PML)\n",
    "    df_report_PML = show_report(kNN_Y_test_PML, kNN_Y_pred_PML, save_path, file_name)\n",
    "    df_report_PML\n",
    "\n",
    "\n",
    "def show_DNN_output(df_extract_train_PML, df_extract_test_PML, save_path, file_name):\n",
    "    # 訓練データを入力したときの出力を可視化\n",
    "    show_2D_metric(df_extract_train_PML, \"PML-train-metric\", save_path, file_name)\n",
    "    # テストデータを入力したときの出力を可視化\n",
    "    show_2D_metric(df_extract_test_PML, \"PML-test-metric\", save_path, file_name)\n",
    "\n",
    "    # 精度評価(2次元ベクトルにkNN)\n",
    "    kNN_Y_test_PML, kNN_Y_pred_PML = apply_kNN(df_extract_train_PML, df_extract_test_PML)\n",
    "    df_report_PML = show_report(kNN_Y_test_PML, kNN_Y_pred_PML, save_path, file_name)\n",
    "    df_report_PML\n",
    "\n",
    "\n",
    "# appの読み込み\n",
    "def read_csv(file_list, label0=True, normalize=True):\n",
    "    count_file = 0\n",
    "    for file in file_list:\n",
    "        if count_file == 0:\n",
    "            count_file += 1\n",
    "            # メモリ不足を防ぐために分割して読み込み\n",
    "            data_reader = pd.read_csv(file, chunksize=500)\n",
    "            df_app = pd.concat((r for r in data_reader), ignore_index=True)\n",
    "            print(\"----------------------\")\n",
    "            print(\"1つ目のデータ\")\n",
    "            print(df_app.iloc[:, 0].value_counts(sort=False))\n",
    "\n",
    "        else:\n",
    "            # メモリ不足を防ぐために分割して読み込み\n",
    "            count_file += 1\n",
    "            data_reader = pd.read_csv(file, chunksize=500)\n",
    "            df_file = pd.concat((r for r in data_reader), ignore_index=True)\n",
    "            print(\"----------------------\")\n",
    "            print(str(count_file) + \"つ目のデータ\")\n",
    "            print(df_file.iloc[:, 0].value_counts(sort=False))\n",
    "            df_app = pd.concat([df_app, df_file])\n",
    "\n",
    "    if normalize:\n",
    "        df_p = df_app.iloc[:, 0:3]\n",
    "        df_q = df_app.iloc[:, 3:]\n",
    "        df_q = (df_q - df_q.min()) / (df_q.max() - df_q.min())\n",
    "\n",
    "        df_app = pd.concat([df_p, df_q], axis=1)\n",
    "        df_app = df_app.fillna(0)\n",
    "\n",
    "    if label0:\n",
    "        # label:0を削減\n",
    "        number = 5000\n",
    "        df_s = df_app.groupby(\"label\").get_group(0).sample(n=number)\n",
    "        df_t = df_app[df_app['label'] != 0]\n",
    "        # 結合\n",
    "        df_app = pd.concat([df_s, df_t])\n",
    "\n",
    "    print(\"----------------------\")\n",
    "    print(\"appの合計のデータ\")\n",
    "    print(df_app.iloc[:, 0].value_counts(sort=False))\n",
    "    print(\"----------------------\")\n",
    "    print(\"最小のlabel数\")\n",
    "    print(df_app['label'].value_counts().min())\n",
    "\n",
    "    return df_app\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a62db-54f2-48da-a61a-ef0a30031097",
   "metadata": {},
   "source": [
    "# 学習実行部\n",
    "\n",
    "file_listには，訓練データのみを使用する．（全体データから，テストデータとなる撮影実験のサンプルを削除するとよい）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ad88a-2a11-4748-9980-30569aafc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全身のオリジナル＋ブラー+明るさ+コントラスト\n",
    "\n",
    "file_list = [\"C:/Users/sugie/PycharmProjects/pythonProject10/orient_tranceformed_data/filltered_feature_all_orient5_num_metric_with_point5_koizumi.csv\"]\n",
    "df_app = read_csv(file_list)\n",
    "\n",
    "# 実行に時間がかかるので注意\n",
    "\n",
    "def make_exp1_aug(df_app):\n",
    "    ### 実験1\n",
    "    # 訓練:exp123/地点123456/被験者oabcdef\n",
    "    # テスト:exp123/地点123456/被験者oabcdef\n",
    "\n",
    "    df_train = df_app\n",
    "\n",
    "    # 最小サンプル数に合わせる\n",
    "    df_train = min_randam_sampling(df_train)\n",
    "\n",
    "    # 訓練とテストに分割\n",
    "    df_train, df_test = train_test_split(df_train, test_size=TRAIN_TEST_RATIO, random_state=0)\n",
    "\n",
    "    # 訓練の中から、テスト画像のaug版を削除する(訓練とテストの実質的な重複を防ぐ)\n",
    "    df_train = del_duplication_data(df_train, df_test)\n",
    "\n",
    "    # 特徴量 (説明変数) 訓練\n",
    "    X_train = df_train.iloc[:, 3:]\n",
    "    # ラベル（目的変数)\n",
    "    y_train = df_train.iloc[:, 0]\n",
    "\n",
    "    # X_yに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=TRAIN_TEST_RATIO, random_state=0,\n",
    "                                                        stratify=y_train)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = make_exp1_aug(df_app)\n",
    "train_loader, val_loader, test_loader = make_dataloader(X_train, X_test, y_train, y_test)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(\"make_exp1_aug 関数を呼び出し中...\")\n",
    "    X_train, X_test, y_train, y_test = make_exp1_aug(df_app)\n",
    "    print(\"make_dataloader 関数を呼び出し中...\")\n",
    "    train_loader, val_loader, test_loader = make_dataloader(X_train, X_test, y_train, y_test)\n",
    "    print(\"run_PML 関数を呼び出し中...\")\n",
    "    run_PML128(\"C:/Users/sugie/PycharmProjects/pythonProject10/モデル保存８\", \"実験1-\"+str(i))\n",
    "    print()\n",
    "    print(str(i) + \"回目が終了しました\")\n",
    "    print(\"###############################################################\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
